import os, time, re, requests, feedparser, pandas as pd
from datetime import datetime, timezone
from deep_translator import GoogleTranslator

GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

# GEMINI_URL = "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent"
# GEMINI_URL = "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent"
# GEMINI_URL = "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-lite:generateContent"
#GEMINI_URL = "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent"
GEMINI_URL = "https://generativelanguage.googleapis.com/v1beta/models/gemini-3-flash-preview:generateContent"



GEMINI_20_FLASH_LIMITS = {
    "MAX_RPM": 14,        # Requests per minute
    "MAX_TPM": 1_000_000, # Tokens per minute
    "MAX_RPD": 200,       # Requests per day
    "TOKEN_ESTIMATE": 350 # Tokens used per request (approx)
}

GEMINI_25_FLASH_LIMITS = {
    "MAX_RPM": 9,         # Requests per minute
    "MAX_TPM": 250_000,    # Tokens per minute
    "MAX_RPD": 250,        # Requests per day
    "TOKEN_ESTIMATE": 350  # Tokens used per request (approx)
}

GEMINI_25_FLASH_LITE_LIMITS = {
    "MAX_RPM": 15,         # Requests per minute
    "MAX_TPM": 250_000,    # Tokens per minute
    "MAX_RPD": 1000,       # Requests per day
    "TOKEN_ESTIMATE": 350  # Tokens per request (approx)
}

GEMINI_3_FLASH_LIMITS = {
    "MAX_RPM": 6,          # ×¤×—×•×ª ×‘×§×©×•×ª ×‘×“×§×”
    "MAX_TPM": 300_000,
    "MAX_RPD": 100,        # free tier ×©××¨× ×™
    "TOKEN_ESTIMATE": 450  # Gemini 3 ××©×ª××© ×‘×™×•×ª×¨ ×˜×•×§× ×™×
}

if "gemini-3" in GEMINI_URL:
    LIMITS = GEMINI_3_FLASH_LIMITS
    print("Using Gemini model: gemini-3-flash-preview", flush=True)
elif "2.5-flash-lite" in GEMINI_URL:
    LIMITS = GEMINI_25_FLASH_LITE_LIMITS
    print("Using Gemini model: gemini-2.5-flash-lite", flush=True)
elif "2.5-flash" in GEMINI_URL:
    LIMITS = GEMINI_25_FLASH_LIMITS
    print("Using Gemini model: gemini-2.5-flash", flush=True)
else:
    LIMITS = GEMINI_20_FLASH_LIMITS
    print("Using Gemini model: gemini-2.0-flash", flush=True)

MAX_RPM = LIMITS["MAX_RPM"] # Requests per minute
MAX_TPM = LIMITS["MAX_TPM"] # Tokens per minute
MAX_RPD = LIMITS["MAX_RPD"] # Requests per day
TOKEN_ESTIMATE = LIMITS["TOKEN_ESTIMATE"]  # Tokens used per request (approx)

OUT_DIR = "data"
OUT_FILE = os.path.join(OUT_DIR, "trending_now_snapshot.csv")

if os.path.exists(OUT_FILE):
    print("ğŸ§¹ Cleaning existing duplicate rows before run...", flush=True)
    df = pd.read_csv(OUT_FILE, encoding="utf-8-sig")

    if not df.empty and all(col in df.columns for col in ["country_en", "title_original", "published"]):
        df["published_date"] = pd.to_datetime(df["published"], errors="coerce").dt.date.astype(str)
        df["dup_key"] = (
            df["country_en"].astype(str).str.strip() + "_" +
            df["title_original"].astype(str).str.strip() + "_" +
            df["published_date"].astype(str).str.strip()
        )

        before = len(df)
        df = df.sort_values(by=["rank"]).drop_duplicates(subset=["dup_key"], keep="first")
        df = df.drop(columns=["dup_key", "published_date"])
        after = len(df)

        if before != after:
            print(f"Removed {before - after} old duplicate rows (same trend & date).", flush=True)
            df.to_csv(OUT_FILE, index=False, encoding="utf-8-sig")
        else:
            print("No duplicates found in existing file.", flush=True)
    else:
        print("Skipped cleanup â€” missing expected columns in existing file.", flush=True)

FEEDS = [
    ("LB", "ar", "Lebanon"),
    ("IL", "he", "Israel"),
    ("PS", "ar", "Gaza"),
    ("SY", "ar", "Syria"),
    ("JO", "ar", "Jordan"),
    ("EG", "ar", "Egypt"),
    ("IR", "fa", "Iran"),
    ("YE", "ar", "Yemen"),
    ("SA", "ar", "Saudi Arabia"),
    ("IQ", "ar", "Iraq"),
    ("AE", "ar", "United Arab Emirates"),
    ("QA", "ar", "Qatar"),
]

FIELDNAMES = [
    "rank", "pulled_at_utc", "country_en",
    "title_original", "title_english",
    "summary_hebrew", "link", "published", "search_volume"
]


translator = GoogleTranslator(source="auto", target="en")
now = datetime.now(timezone.utc).isoformat()

def is_english(text):
    return bool(re.fullmatch(r"[A-Za-z0-9\s.,!?'\-]+", text.strip()))

def preserve_numbers(text):
    return re.sub(r"(\d+)", r"Â¤\1Â¤", text)

def restore_numbers(translated_text):
    return re.sub(r"Â¤(\d+)Â¤", r"\1", translated_text)

def safe_translate(text, retries=3):
    if not text or not text.strip():
        return ""
    if is_english(text):
        return text
    text = preserve_numbers(text)
    for _ in range(retries):
        try:
            translated = translator.translate(text)
            return restore_numbers(translated)
        except Exception:
            time.sleep(1)
    return restore_numbers(text)

last_requests = []
total_tokens_this_min = 0

def respect_rate_limits(tokens_used):
    global last_requests, total_tokens_this_min

    now = time.time()
    last_requests = [(t, tok) for t, tok in last_requests if now - t < 60]
    total_tokens_this_min = sum(tok for _, tok in last_requests)

    if len(last_requests) >= MAX_RPM:
        sleep_time = 60 - (now - last_requests[0][0])
        print(f"Reached RPM limit ({MAX_RPM}). Sleeping {sleep_time:.1f}s...")
        time.sleep(max(1, sleep_time))

    if total_tokens_this_min + tokens_used > MAX_TPM:
        print("Reached TPM limit. Waiting 60s...", flush=True)
        time.sleep(60)
        total_tokens_this_min = 0
        last_requests = []

    last_requests.append((now, tokens_used))

def summarize_with_gemini(trend, country, retries=1):
    if not GEMINI_API_KEY:
        return "Missing GEMINI_API_KEY"

    prompt = f"""
×§×¨××ª ×¢×›×©×™×• ×¢×œ ×”×˜×¨× ×“ "{trend}" ×©×¢×œ×” ×‘×©×¢×•×ª ×”××—×¨×•× ×•×ª ×‘××“×™× ×ª "{country}".
×¡×›× ××•×ª×• ×‘×¢×‘×¨×™×ª ×˜×‘×¢×™×ª ×•×‘×¨×•×¨×”, ×‘×©×œ×•×© ×©×•×¨×•×ª ×‘×œ×‘×“:

1ï¸×›×ª×•×‘ ×‘×©×•×¨×” ××—×ª ××”×• × ×•×©× ×”×˜×¨× ×“ (×œ×“×•×’××”: ×¡×¤×•×¨×˜, ×¤×•×œ×™×˜×™×§×”, ×ª×¨×‘×•×ª, ××™×©×™×•×ª ××¤×•×¨×¡××ª ×•×›×•').
2ï¸ ×›×ª×•×‘ ×‘×©×•×¨×” ××—×ª ××” ×§×¨×” ××• ××” ××¢×•×¨×¨ ×¢× ×™×™×Ÿ ×‘×˜×¨× ×“ ×”×–×” ×‘××“×™× ×”.
3ï¸ ×× ××¤×©×¨, ×”×•×¡×£ ×‘×©×•×¨×” ×”×©×œ×™×©×™×ª ×”×§×©×¨ ×§×¦×¨ â€“ ×œ××” ×× ×©×™× ××“×‘×¨×™× ×¢×œ ×–×” ×›×¢×ª.

× × ×›×ª×•×‘ ×‘×¡×’× ×•×Ÿ ×ª×§×©×•×¨×ª×™, ×× ×•×©×™ ×•×¢× ×™×™× ×™ (×œ× ×›××• ×ª×¨×’×•× ××›×•× ×” ××• ×¨×©×™××” ×™×‘×©×”).
××œ ×ª×©×ª××© ×‘××™×œ×™× ×›××• "×”×˜×¨× ×“ ×”×•×..." ××• "×”× ×•×©× ×¢×•×¡×§ ×‘...", ××œ× ×™×©×¨ ×‘×ª×•×›×Ÿ.
×¢× ×” ×¨×§ ×‘×˜×§×¡×˜ ×¨×’×™×œ, ×œ×œ× Markdown, ×œ×œ× ×›×•×ª×¨×•×ª ×•×œ×œ× ××™××•×’×³×™×.
"""
    payload = {"contents": [{"parts": [{"text": prompt}]}]}
    headers = {"x-goog-api-key": GEMINI_API_KEY}

    for attempt in range(1, retries + 1):
        respect_rate_limits(TOKEN_ESTIMATE)
        try:
            r = requests.post(GEMINI_URL, headers=headers, json=payload, timeout=(5, 20))
            data = r.json()
            text = data.get("candidates", [{}])[0].get("content", {}).get("parts", [{}])[0].get("text", "")
            if text and "No response" not in text:
                return text.strip()
        except Exception as e:
            print(f"Attempt {attempt} failed for '{trend}': {e}", flush=True)
        wait_time = min(30, attempt * 10)
        time.sleep(wait_time)

    return "âŒ No response"

os.makedirs(OUT_DIR, exist_ok=True)
rows = []

for geo, lang, country_en in FEEDS:
    print(f"\nğŸŒ Country start: {country_en} ({geo})", flush=True)
    url = f"https://trends.google.com/trending/rss?geo={geo}&hl={lang}"
    d = feedparser.parse(url)
    
    total_trends = len(d.entries)
    print(f"   Found {total_trends} trends", flush=True)
    
    for idx, e in enumerate(d.entries, start=1):
        title_raw = e.title.strip()
        title_translated = safe_translate(title_raw)
    
        print(
            f"   â†’ [{country_en}] ({idx}/{total_trends}) Start: {title_translated[:60]}",
            flush=True
        )
    
        summary = summarize_with_gemini(title_translated, country_en)
    
        print(
            f"   âœ“ [{country_en}] ({idx}/{total_trends}) Done",
            flush=True
        )
        rows.append({
            "pulled_at_utc": now,
            "country_en": country_en,
            "title_original": title_raw,
            "title_english": title_translated,
            "summary_hebrew": summary,
            "link": getattr(e, "link", ""),
            "published": getattr(e, "published", ""),
            "search_volume": traffic_clean,
        })
        time.sleep(1)

if os.path.exists(OUT_FILE):
    old_df = pd.read_csv(OUT_FILE, encoding="utf-8-sig")
    old_df["rank"] = old_df["rank"] + 1
    new_df = pd.DataFrame(rows)
    new_df.insert(0, "rank", 1)
    combined = pd.concat([new_df, old_df], ignore_index=True)
    
    # delete at 4.11.25 because sometimes the date retrieved isnt at the format we need.
    # combined["published_date"] = pd.to_datetime(combined["published"], errors="coerce").dt.date.astype(str)

    # add at 4.11.25
    # Convert 'published' column safely to datetime
    combined["published"] = pd.to_datetime(
        combined["published"],
        errors="coerce",                # ×¢×¨×›×™× ×œ× ×ª×§×™× ×™× â†’ NaT ×‘××§×•× ×œ×”×¤×™×œ ××ª ×”×§×•×“
        utc=True,
        infer_datetime_format=True      # ××–×”×” ×¤×•×¨××˜×™× ×›××• Fri, 17 Oct 2025 07:30:00 -0700
    )
    
    # Check if any invalid dates found
    invalid_count = combined["published"].isna().sum()
    if invalid_count > 0:
        print(f"âš ï¸ Found {invalid_count} invalid 'published' values that could not be parsed as dates.", flush=True)
    
    # Extract just the date part
    combined["published_date"] = combined["published"].dt.date.astype(str).fillna("unknown")

    combined["dup_key"] = (
        combined["country_en"].astype(str).str.strip() + "_" +
        combined["title_original"].astype(str).str.strip() + "_" +
        combined["published_date"].astype(str).str.strip()
        )
    combined = combined.sort_values(by=["rank"]).drop_duplicates(subset=["dup_key"], keep="first")
    combined = combined.drop(columns=["dup_key", "published_date"])

else:
    new_df = pd.DataFrame(rows)
    new_df.insert(0, "rank", 1)
    combined = new_df

print("\nChecking for previous 'No response' summaries...", flush=True)
retry_mask = (combined["summary_hebrew"].astype(str).str.contains("No response")) | \
             (combined["summary_hebrew"].astype(str).str.startswith("âš ï¸"))
no_response_df = combined[retry_mask].copy()

if not no_response_df.empty:
    print(f"Found {len(no_response_df)} items without summary. Retrying Gemini up to 3 times...", flush=True)
    for idx, row in no_response_df.iterrows():
        trend = row["title_english"]
        country = row["country_en"]
        new_summary = summarize_with_gemini(trend, country, retries=3)
        combined.at[idx, "summary_hebrew"] = new_summary
        print(f"â†» Retried: {trend[:50]} â†’ {new_summary[:40]}", flush=True)
else:
    print("No missing summaries found.", flush=True)

combined.to_csv(OUT_FILE, index=False, encoding="utf-8-sig")

print(f"\nSaved {len(rows)} new trends with summaries.", flush=True)
print(f"Output file: {os.path.abspath(OUT_FILE)}", flush=True)
print(f"Total rows: {len(combined)}", flush=True)
